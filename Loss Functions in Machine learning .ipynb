{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS FUNCTIONS\n",
    "\n",
    "Loss functions quantify how close a given neural network is to the ideal toward which it is training. The idea is simple. We calculate a metric based on the error we observe in the network’s predictions. We then aggregate these errors over the entire dataset and average them and now we have a single number representative of how close the neural network is to its ideal. Looking for this ideal state is equivalent to finding the parameters (weights and biases) that will minimize the “loss” incurred from the errors. In this way, loss functions help reframe training neural networks as an optimization problem. In most cases, these parameters cannot be solved for analytically, but, more often than not, they can be approximated well with iterative optimization algorithms like gradient descent. The following section provides an overview on commonly seen loss functions, linking them back to their origins in machine learning, as necessary.\n",
    "<img src=\"loss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meansquarederror(ytrue,ypred):\n",
    "    trainingexamples = ytrue.shape[0]\n",
    "    print(\"total no of training examples is\"+str(trainingexamples))\n",
    "    errorvector = ypred-ytrue\n",
    "    print(\"shape of error vector\"+str(errorvector.shape))\n",
    "    error = (1/trainingexamples)*np.sum(errorvector**2)\n",
    "    return error\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "genearatetrainingsets = np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "genearatetrainingsetspred = np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of training examples is100\n",
      "shape of error vector(100, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.1300515122978085"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meansquarederror(genearatetrainingsets,genearatetrainingsetspred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is MSE Really a Convex Loss Function? \n",
    "\n",
    "In a technical sense, the MSE is a convex loss function. However, when dealing with hidden layers in neural networks, the convex property no longer holds true, because we could have multiple parameter sets of values resulting in the same loss value.\n",
    "### Optimizing MSE \n",
    "Optimizing the MSE is equivalent to optimizing for the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean absolute error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanabsoluteerror(ytrue,ypred):\n",
    "    trainingexamples = ytrue.shape[0]\n",
    "    print(\"total no of training examples is\"+str(trainingexamples))\n",
    "    errorvector = ypred-ytrue\n",
    "    print(\"shape of error vector\"+str(errorvector.shape))\n",
    "    error = (1/trainingexamples)*np.sum(np.abs(errorvector))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of training examples is100\n",
      "shape of error vector(100, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1399313206114992"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanabsoluteerror(genearatetrainingsets,genearatetrainingsetspred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean squared log error los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meansquarelogerror(ytrue,ypred):\n",
    "    trainingexamples = ytrue.shape[0]\n",
    "    print(\"total no of training examples is\"+str(trainingexamples))\n",
    "    errorvector = np.log(np.abs(ypred))-np.log(np.abs(ytrue))\n",
    "    print(\"shape of error vector\"+str(errorvector.shape))\n",
    "    error = (1/trainingexamples)*np.sum(errorvector**2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of training examples is100\n",
      "shape of error vector(100, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.6846011395015252"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meansquarelogerror(genearatetrainingsets,genearatetrainingsetspred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean absolute percentage error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanabsolutepercentageloss(ytrue,ypred):\n",
    "    trainingexamples = ytrue.shape[0]\n",
    "    print(\"total no of training examples is\"+str(trainingexamples))\n",
    "    errorvector = 100*(np.abs(ypred-ytrue)/np.abs(ytrue))\n",
    "    print(\"shape of error vector\"+str(errorvector.shape))\n",
    "    error = (1/trainingexamples)*np.sum(errorvector)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of training examples is100\n",
      "shape of error vector(100, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1860.9473782172968"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanabsolutepercentageloss(genearatetrainingsets,genearatetrainingsetspred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge loss \n",
    "Hinge loss is the most commonly used loss function when the network must be optimized for a hard classification. For example, 0 = no fraud and 1 = fraud, which by convention is called a 0-1 classifier. The 0,1 choice is somewhat arbitrary and –1, 1 is also seen in lieu of 0–1. Hinge loss is also seen in a class of models called maximummargin classification models (e.g., support vector machines, a somewhat distant cousin to neural networks). \n",
    "The hinge loss is mostly used for binary classifications. There are extensions for multiclass classification (e.g., “one versus all,” “one versus one”) for the hinge loss that are not covered here.\n",
    "#### Hinge Loss Is a Convex Function \n",
    "Note that like the MSE, the hinge loss is known to be a convex function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hingeloss(ytrue,ypred):\n",
    "    createarray =[]\n",
    "    trainingexamples = ytrue.shape[0]\n",
    "    createzeros = np.zeros(trainingexamples)\n",
    "    print(\"total no of training examples is\"+str(trainingexamples))\n",
    "    errorvector=1-ypred*ytrue\n",
    "    #print( errorvector)\n",
    "    for i in errorvector:\n",
    "        #print(i)\n",
    "        if i >0:\n",
    "            createarray.append(i)\n",
    "        else:\n",
    "            createarray.append(0)\n",
    "    createarray = np.array(createarray)\n",
    "    #print(createarray)\n",
    "    #errorvector = np.maximum(createzeros,errorvector)\n",
    "    print(\"shape of error vector\"+str(createarray.shape))\n",
    "    error = (1/trainingexamples)*np.sum(createarray)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of training examples is100\n",
      "shape of error vector(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.1800846])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hingeloss(genearatetrainingsets,genearatetrainingsetspred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic loss\n",
    "Logistic loss functions are used when probabilities are of greater interest than hard classifications. Great examples of these would be flagging potential fraud, with a human-in-the-loop solution or predicting the “probability of someone clicking on an ad,” which can then be linked to a currency number.\n",
    "Predicting valid probabilities means generating numbers between 0 and 1. Predicting valid probabilities also means making sure the probability of mutually exclusive outcomes should sum to one. For this reason, it is essential that the very last layer of a neural network used in classification is a softmax. Note that the sigmoid activation function also will give valid values between 0 and 1. However, you cannot use it for scenarios in which the outputs are mutually exclusive, because it does not model the dependencies between the output values. Now that we have made sure our neural network will produce valid probabilities for the classes we have, we can dive headlong into the loss function and into the idea of what we should be optimizing here. We want to optimize for what is formally called the “maximum likelihood.” In other words, we want to maximize the probability we predict for the correct class AND we want to do so for every single sample we have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative log likelihood. \n",
    "For the sake of mathematical convenience, when dealing with the product of probabilities, it is customary to convert them to the log of the probabilities; hence, the product of the probabilities transforms to the sum of the log of the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Log Likelihood and Maximizing Probability\n",
    "The logarithm is a monotonically increasing function. Thus, minimizing the negative log likelihood is equivalent to maximizing the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(arrays):\n",
    "    return 1/(1+np.exp((-1)*arrays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigtrain=sigmoid(genearatetrainingsets)\n",
    "sigpred=sigmoid(genearatetrainingsetspred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticsloss(ytrue,ypred):\n",
    "    trainingexamples = ytrue.shape[0]\n",
    "    errorvector  = (ytrue*np.log(ypred))+(ytrue*np.log(1-ypred))\n",
    "    return (1/trainingexamples)*np.sum(errorvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8463647615390332"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticsloss(sigtrain,sigpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy\n",
    "Cross-entropy has its origin in information theory, whereas negative log likelihood for classification has its origin in statistical modeling. the cross-entropy between two probability distributions—in this case, what we predict and what we have observed under the same criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(ytrue,ypred):\n",
    "    trainingexamples = ytrue.shape[0]\n",
    "    errorvector = -1 * ytrue * np.log(ypred)\n",
    "    return (1/trainingexamples)*np.sum(errorvector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.441498924599787"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossentropy(sigtrain,sigpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions for Reconstruction \n",
    "This set of loss functions relates to what is called reconstruction. The idea is simple. A neural network is trained to recreate its input as closely as possible. So, why is this any different from memorizing the entire dataset? The key here is to tweak the scenario so that the network is forced to learn commonalities and features across the dataset. In one approach, the number of parameters in the network is constrained such that the network is forced to compress the data and then re-create it. Another often-used approach is to corrupt the input with meaningless “noise” and train the network to ignore the noise and learn the data. Examples of these kinds of neural nets are restricted Boltzmann machines, autoencoders, and so on. These neural networks all use loss functions that are rooted in information theory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction(ytrue,ypred):\n",
    "    trainingexamples = ytrue.shape[0]\n",
    "    errorvector = -1 * ytrue * np.log(ypred)\n",
    "    return (1/trainingexamples)*np.sum(errorvector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
